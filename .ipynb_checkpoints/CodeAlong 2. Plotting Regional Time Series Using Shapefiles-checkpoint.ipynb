{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11c1f4d0-60ad-42b2-9a7d-bfafc254fde2",
   "metadata": {},
   "source": [
    "# Plotting Time Series from Multiple Ensemble Members\n",
    "## (using Regions Defined by Shape Files!)\n",
    "### Authors\n",
    "\n",
    "Samantha Stevenson sstevenson@ucsb.edu\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "[Goals](#purpose)\n",
    "\n",
    "[Import Packages](#path)\n",
    "\n",
    "[Load and Query the CMIP6 AWS Catalog](#load)\n",
    "\n",
    "[Read in Data as an Xarray Object](#xarray)\n",
    "\n",
    "[Define a Region Using Shapefiles](#shapefiles)\n",
    "\n",
    "[Plot Time Series](#time_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1e5e3f-d7ff-4429-a986-348efdc2f005",
   "metadata": {},
   "source": [
    "<a id='purpose'></a> \n",
    "## **Goals**\n",
    "\n",
    "In this tutorial, we will be reading in the database of Coupled Model Intercomparison Project phase 6 (CMIP6) output hosted by Amazon Web Services and exploring its contents. \n",
    "\n",
    "The steps in this tutorial build on the skills we learned in previous tutorials:\n",
    "- [Read in Data and Plot a Time Series](https://github.com/climate-datalab/Time-Series-Plots/blob/main/1.%20Read%20in%20Climate%20Data%20%2B%20Plot%20a%20Regionally%20Averaged%20Time%20Series.ipynb)\n",
    "  (regional averaging, time series plotting)\n",
    "- [Opening and Querying the CMIP6 AWS Database](https://github.com/climate-datalab/CMIP6_AWS/blob/main/1.%20Opening%20and%20Querying%20the%20CMIP6%20Catalog.ipynb)  (data access via Amazon Web Services)\n",
    "- [Mapping Climate Data](https://github.com/climate-datalab/Map-Plots/blob/main/1.%20Mapping%20Climate%20Data.ipynb) (putting spatial data onto a map using Cartopy)\n",
    "\n",
    "The first few sections of this tutorial contain the same coding steps as [tutorial 1 in this repo](https://github.com/climate-datalab/EnsembleAnalysis/blob/main/1.%20Plotting%20Multiple%20Ensemble%20Members.ipynb), but then we build on this to illustrate how one can use shape files to extract data from spatially irregular regions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13901a3c-886b-46be-9142-985e660a00fe",
   "metadata": {},
   "source": [
    "<a id='path'></a> \n",
    "## **Import Packages**\n",
    "\n",
    "As always, we begin by importing the necessary packages for our analysis. This tutorial assumes you're starting with an environment in which `intake`, `intake-esm`, and `s3fs` are already installed - for details on those packages, see the [CMIP6 AWS repo](https://github.com/climate-datalab/CMIP6_AWS)!\n",
    "\n",
    "We'll also need a new package for this tutorial: `geopandas`. [Geopandas](https://geopandas.org/en/stable/index.html) is designed to facilitate working with geospatial data in Python; it layers the functionality of Pandas with the shape-handing abilities of [Shapely](https://pypi.org/project/shapely/) to allow users to perform operations on geometrics objects. \n",
    "\n",
    "Last but not least: we'll also import the coordinate reference system handling functionality from Cartopy (`cartopy.crs`; for more details see the [Cartopy CRS docs page](https://scitools.org.uk/cartopy/docs/latest/getting_started/crs.html)). This will allow us to reproject geospatial data onto a given CRS using Geopandas later on! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "522ca01c-9b61-497e-b15e-d9453b6674ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR 1: PROJ: proj_create_from_database: Open of /opt/anaconda3/envs/eds296-stevenson/share/proj failed\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import intake\n",
    "import s3fs\n",
    "import geopandas as gpd\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5641f65b-5dac-441f-b966-720b1b1c70ea",
   "metadata": {},
   "source": [
    "<a id='load'></a> \n",
    "## **Load and Query the CMIP6 AWS Catalog**\n",
    "\n",
    "As we did in previous tutorials, we'll load the CMIP6 database hosted by Amazon Web Services. More detail on that database is available on the [Amazon Registry of Open Data](https://registry.opendata.aws/cmip6/).\n",
    "\n",
    "\n",
    "We first begin with opening the data catalog itself (_note - this step can be fairly slow on some machines_):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53f17dab-cef6-4a2a-a5c7-8e4ef375f462",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open the CMIP6 data catalog, store as a variable\n",
    "catalog = intake.open_esm_datastore('https://cmip6-pds.s3.amazonaws.com/pangeo-cmip6.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882f31b9-a0d9-4182-a00d-c6c1e6c6b34b",
   "metadata": {},
   "source": [
    "We'll use the same query that we constructed earlier: pulling all members of the historical ensemble run with CESM2. For more detail on this, see [tutorial 1](https://github.com/climate-datalab/EnsembleAnalysis/blob/main/1.%20Plotting%20Multiple%20Ensemble%20Members.ipynb)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "279b49ac-00bb-43fd-9e62-eada346435a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify search terms to query catalog \n",
    "# activity_id: which project do you want? CMIP = historical data\n",
    "activity_ids = ['CMIP'] \n",
    "\n",
    "# source_id: which model do you want? Let's say CESM2\n",
    "source_id = ['CESM2']\n",
    "\n",
    "# experiment_id: what experimental configuration do you want? Here we want historical and the four main SSPs\n",
    "experiment_ids = ['historical']\n",
    "\n",
    "# table_id: which part of the Earth system and time resolution do you want? Here we want monthly atmosphere data\n",
    "table_id = 'Amon' \n",
    "\n",
    "# variable_id: which climate variable do you want? Here we want surface air temperature\n",
    "variable_id = 'tas' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25343565-9698-4b67-bbf7-494502972ed7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activity_id</th>\n",
       "      <th>institution_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>member_id</th>\n",
       "      <th>table_id</th>\n",
       "      <th>variable_id</th>\n",
       "      <th>grid_label</th>\n",
       "      <th>zstore</th>\n",
       "      <th>dcpp_init_year</th>\n",
       "      <th>version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CMIP</td>\n",
       "      <td>NCAR</td>\n",
       "      <td>CESM2</td>\n",
       "      <td>historical</td>\n",
       "      <td>r4i1p1f1</td>\n",
       "      <td>Amon</td>\n",
       "      <td>tas</td>\n",
       "      <td>gn</td>\n",
       "      <td>s3://cmip6-pds/CMIP6/CMIP/NCAR/CESM2/historica...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20190308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CMIP</td>\n",
       "      <td>NCAR</td>\n",
       "      <td>CESM2</td>\n",
       "      <td>historical</td>\n",
       "      <td>r6i1p1f1</td>\n",
       "      <td>Amon</td>\n",
       "      <td>tas</td>\n",
       "      <td>gn</td>\n",
       "      <td>s3://cmip6-pds/CMIP6/CMIP/NCAR/CESM2/historica...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20190308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CMIP</td>\n",
       "      <td>NCAR</td>\n",
       "      <td>CESM2</td>\n",
       "      <td>historical</td>\n",
       "      <td>r3i1p1f1</td>\n",
       "      <td>Amon</td>\n",
       "      <td>tas</td>\n",
       "      <td>gn</td>\n",
       "      <td>s3://cmip6-pds/CMIP6/CMIP/NCAR/CESM2/historica...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20190308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CMIP</td>\n",
       "      <td>NCAR</td>\n",
       "      <td>CESM2</td>\n",
       "      <td>historical</td>\n",
       "      <td>r1i1p1f1</td>\n",
       "      <td>Amon</td>\n",
       "      <td>tas</td>\n",
       "      <td>gn</td>\n",
       "      <td>s3://cmip6-pds/CMIP6/CMIP/NCAR/CESM2/historica...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20190308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CMIP</td>\n",
       "      <td>NCAR</td>\n",
       "      <td>CESM2</td>\n",
       "      <td>historical</td>\n",
       "      <td>r2i1p1f1</td>\n",
       "      <td>Amon</td>\n",
       "      <td>tas</td>\n",
       "      <td>gn</td>\n",
       "      <td>s3://cmip6-pds/CMIP6/CMIP/NCAR/CESM2/historica...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20190308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CMIP</td>\n",
       "      <td>NCAR</td>\n",
       "      <td>CESM2</td>\n",
       "      <td>historical</td>\n",
       "      <td>r5i1p1f1</td>\n",
       "      <td>Amon</td>\n",
       "      <td>tas</td>\n",
       "      <td>gn</td>\n",
       "      <td>s3://cmip6-pds/CMIP6/CMIP/NCAR/CESM2/historica...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20190308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CMIP</td>\n",
       "      <td>NCAR</td>\n",
       "      <td>CESM2</td>\n",
       "      <td>historical</td>\n",
       "      <td>r9i1p1f1</td>\n",
       "      <td>Amon</td>\n",
       "      <td>tas</td>\n",
       "      <td>gn</td>\n",
       "      <td>s3://cmip6-pds/CMIP6/CMIP/NCAR/CESM2/historica...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20190311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CMIP</td>\n",
       "      <td>NCAR</td>\n",
       "      <td>CESM2</td>\n",
       "      <td>historical</td>\n",
       "      <td>r8i1p1f1</td>\n",
       "      <td>Amon</td>\n",
       "      <td>tas</td>\n",
       "      <td>gn</td>\n",
       "      <td>s3://cmip6-pds/CMIP6/CMIP/NCAR/CESM2/historica...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20190311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CMIP</td>\n",
       "      <td>NCAR</td>\n",
       "      <td>CESM2</td>\n",
       "      <td>historical</td>\n",
       "      <td>r7i1p1f1</td>\n",
       "      <td>Amon</td>\n",
       "      <td>tas</td>\n",
       "      <td>gn</td>\n",
       "      <td>s3://cmip6-pds/CMIP6/CMIP/NCAR/CESM2/historica...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20190311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CMIP</td>\n",
       "      <td>NCAR</td>\n",
       "      <td>CESM2</td>\n",
       "      <td>historical</td>\n",
       "      <td>r10i1p1f1</td>\n",
       "      <td>Amon</td>\n",
       "      <td>tas</td>\n",
       "      <td>gn</td>\n",
       "      <td>s3://cmip6-pds/CMIP6/CMIP/NCAR/CESM2/historica...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20190313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CMIP</td>\n",
       "      <td>NCAR</td>\n",
       "      <td>CESM2</td>\n",
       "      <td>historical</td>\n",
       "      <td>r11i1p1f1</td>\n",
       "      <td>Amon</td>\n",
       "      <td>tas</td>\n",
       "      <td>gn</td>\n",
       "      <td>s3://cmip6-pds/CMIP6/CMIP/NCAR/CESM2/historica...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20190514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   activity_id institution_id source_id experiment_id  member_id table_id  \\\n",
       "0         CMIP           NCAR     CESM2    historical   r4i1p1f1     Amon   \n",
       "1         CMIP           NCAR     CESM2    historical   r6i1p1f1     Amon   \n",
       "2         CMIP           NCAR     CESM2    historical   r3i1p1f1     Amon   \n",
       "3         CMIP           NCAR     CESM2    historical   r1i1p1f1     Amon   \n",
       "4         CMIP           NCAR     CESM2    historical   r2i1p1f1     Amon   \n",
       "5         CMIP           NCAR     CESM2    historical   r5i1p1f1     Amon   \n",
       "6         CMIP           NCAR     CESM2    historical   r9i1p1f1     Amon   \n",
       "7         CMIP           NCAR     CESM2    historical   r8i1p1f1     Amon   \n",
       "8         CMIP           NCAR     CESM2    historical   r7i1p1f1     Amon   \n",
       "9         CMIP           NCAR     CESM2    historical  r10i1p1f1     Amon   \n",
       "10        CMIP           NCAR     CESM2    historical  r11i1p1f1     Amon   \n",
       "\n",
       "   variable_id grid_label                                             zstore  \\\n",
       "0          tas         gn  s3://cmip6-pds/CMIP6/CMIP/NCAR/CESM2/historica...   \n",
       "1          tas         gn  s3://cmip6-pds/CMIP6/CMIP/NCAR/CESM2/historica...   \n",
       "2          tas         gn  s3://cmip6-pds/CMIP6/CMIP/NCAR/CESM2/historica...   \n",
       "3          tas         gn  s3://cmip6-pds/CMIP6/CMIP/NCAR/CESM2/historica...   \n",
       "4          tas         gn  s3://cmip6-pds/CMIP6/CMIP/NCAR/CESM2/historica...   \n",
       "5          tas         gn  s3://cmip6-pds/CMIP6/CMIP/NCAR/CESM2/historica...   \n",
       "6          tas         gn  s3://cmip6-pds/CMIP6/CMIP/NCAR/CESM2/historica...   \n",
       "7          tas         gn  s3://cmip6-pds/CMIP6/CMIP/NCAR/CESM2/historica...   \n",
       "8          tas         gn  s3://cmip6-pds/CMIP6/CMIP/NCAR/CESM2/historica...   \n",
       "9          tas         gn  s3://cmip6-pds/CMIP6/CMIP/NCAR/CESM2/historica...   \n",
       "10         tas         gn  s3://cmip6-pds/CMIP6/CMIP/NCAR/CESM2/historica...   \n",
       "\n",
       "    dcpp_init_year   version  \n",
       "0              NaN  20190308  \n",
       "1              NaN  20190308  \n",
       "2              NaN  20190308  \n",
       "3              NaN  20190308  \n",
       "4              NaN  20190308  \n",
       "5              NaN  20190308  \n",
       "6              NaN  20190311  \n",
       "7              NaN  20190311  \n",
       "8              NaN  20190311  \n",
       "9              NaN  20190313  \n",
       "10             NaN  20190514  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Search through catalog, store results in \"res\" variable\n",
    "res = catalog.search(activity_id=activity_ids, source_id=source_id, experiment_id=experiment_ids, \n",
    "                     table_id=table_id, variable_id=variable_id)\n",
    "\n",
    "# Display data frame associated with results\n",
    "display(res.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967e9b9f-1dcb-4ee9-a8aa-4aa8884b5e98",
   "metadata": {},
   "source": [
    "Once the data frame with our search query results is constructed, we can loop through the members and create an xarray object that contains all the information in a nice, easy-to-use format. Again, this is explained in more gory detail in tutorial 1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3781bb06-109f-4ee7-82dc-a2519ab57266",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r4i1p1f1\n",
      "r6i1p1f1\n",
      "r3i1p1f1\n",
      "r1i1p1f1\n",
      "r2i1p1f1\n",
      "r5i1p1f1\n",
      "r9i1p1f1\n",
      "r8i1p1f1\n",
      "r7i1p1f1\n",
      "r10i1p1f1\n",
      "r11i1p1f1\n"
     ]
    }
   ],
   "source": [
    "# Define an empty list\n",
    "ens_data = []\n",
    "\n",
    "# Retrieve number of entries in the data frame\n",
    "num = res.df.shape[0]\n",
    "\n",
    "# Loop over all entries in the data frame\n",
    "for mem in range(num):\n",
    "    print(res.df.member_id[mem])\n",
    "    # Store data from each entry as xarray, add to list\n",
    "    temp_data = xr.open_zarr(res.df['zstore'][mem], storage_options={'anon': True})\n",
    "    ens_data.append(temp_data)\n",
    "\n",
    "# Concatenate the list into a single xarray object\n",
    "ens_data = xr.concat(ens_data, dim=\"member\")\n",
    "\n",
    "# Store the actual member information as values of the new dimension\n",
    "ens_data = ens_data.assign_coords(member=(\"member\", res.df.member_id))\n",
    "\n",
    "# Convert units from K to C\n",
    "ens_data['tas'] = ens_data['tas'] - 273.15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef232f75-5bf9-4fb0-a3ff-c1bd14569a5e",
   "metadata": {},
   "source": [
    "<a id='shapefiles'></a> \n",
    "## **Define a Region Using Shapefiles**\n",
    "\n",
    "Now that the data have been read in, we can use it to plot a time series. In previous tutorials, we had been specifying lat/lon ranges using a rectangular box: but we can do better now! A common desire in analyzing geospatial data is to select regions with irregular boundaries - this is often done using shapefiles which specify the lat/lon coordinates of the boundary around a given region. \n",
    "\n",
    "There are many sources of shapefiles around the Internet: here we'll work with the [California Geographic Boundaries](https://catalog.data.gov/dataset/ca-geographic-boundaries) datasets. These contain information for state, county, and local place boundaries - to make sure we have a large enough region, let's use the state boundary. \n",
    "\n",
    "The shape file for the California state boundary was downloaded from the link above and is stored in this repo (see folder \"ca_state\"). It can be read in using the Geopandas `.read_file()` method!\n",
    "\n",
    "While we're at it, let's also reproject the file to use a specific coordinate reference system - in this case, the [Plate Carree](https://pro.arcgis.com/en/pro-app/latest/help/mapping/properties/plate-carree.htm) projection. A CRS is essentially a framework for locating different spatial points on the surface of Earth; more information on coordinate reference systems can be found [here](https://www.earthdatascience.org/courses/earth-analytics/spatial-data-r/intro-to-coordinate-reference-systems/)\n",
    "\n",
    "_Note: the `epsg=4326` syntax below is how you refer to the Plate Carree projection in the language of Geopandas! The [EPSG database](https://epsg.org/home.html) has numbered various projections/CRS, and 4326 is the one that corresponds to Plate Carree._\n",
    "\n",
    "This reprojection step isn't always required since the shape file does contain a default CRS, but we _will_ need to make sure in a minute that our CRS is consistent between the shape file and the climate model data, so we might as well explicitly include a reprojection step just to make sure we don't forget to check! \n",
    "\n",
    "Another side benefit to this: part of the sanity checking process below also involves plotting the data on a map, to make sure everything works correctly. That means that we ALSO need to make sure that our PLOTTING map CRS is consistent with the shape file, so we definitely need to keep track of what CRS we're working with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7b47c0d-4015-4e60-8c95-82e27b469513",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read in shapefile for CA counties\n",
    "gdf = gpd.read_file('ca_state/CA_State.shp')\n",
    "\n",
    "# Reproject the shapefile to use the PlateCarree projection\n",
    "gdf = gdf.to_crs(epsg = 4326)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dcc922-4b43-46ee-987a-f83ab34378d3",
   "metadata": {},
   "source": [
    "Now that we have our shape file, the next task is to take the lat and lon coordinates from the climate model grid, and figure out which of those points lie within the boundaries of the shape (in this case, the California state borders). \n",
    "\n",
    "In CESM2, the `lon` and `lat` coordinates are provided as _one-dimensional variables_: just an individual list of either longitude or latitude values. But in order for us to look at whether a point is inside or outside of our California shape, we need a _two-dimensional grid_ of lat and lon values! In other words: we need to combine the lats and lons to get all the possible combinations that could happen.\n",
    "\n",
    "Numpy has a function to take care of this kind of problem, called `meshgrid`, which takes 1D vectors of latitude and longitude and converts it to a grid. Let's check it out: while we're at it, we'll also use Numpy's `where` function to convert our longitudes from the 0-360E convention used by CESM2, to the -180 to 180 convention used in Shapely/Geopandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6da12878-45ca-44fe-abfe-40f9acc70e22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make 2D lat, lon\n",
    "lon_vals = ens_data.lon.values\n",
    "lat_vals = ens_data.lat.values\n",
    "lon_vals = np.where(lon_vals > 180, lon_vals - 360, lon_vals)\n",
    "\n",
    "# Convert values of longitude greater than 180 to negative values\n",
    "lon2d, lat2d = np.meshgrid(lon_vals, lat_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c4757b-1cb6-4c92-bc39-3ba605a77860",
   "metadata": {},
   "source": [
    "Great, now we have our grid of points - next we still need to compare them with the California state boundaries. A good way to do this is by creating a set of `Points` objects using the `shapely` Python package: this will turn our lat/lon grid into a list of all possible geospatial points that occur in the CESM2 grid.\n",
    "\n",
    "The line of code below is doing a few different things all at once:\n",
    "- Using `flatten` to convert the 2D lat/lon arrays back to vectors - but now vectors that contain ALL the different lat/lon combinations\n",
    "- Feeding the result of `flatten` to the `zip` command, which pairs the lat and lon vectors together into a set of tuples\n",
    "- Converting each individual tuple value into a Shapely point, and sticking them together into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7aa740bb-57ba-4bc8-b665-47eb11090b91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build a list of 'points' from the CESM2 grid\n",
    "points = [Point(lon, lat) for lon, lat in zip(lon2d.flatten(), lat2d.flatten())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3dd028-cb5c-4634-aa78-679beade9579",
   "metadata": {},
   "source": [
    "From this list of points, we can concatenate them together into a Geopandas GeoDataFrame object! This allows Geopandas to more easily manipulate the geospatial data, and project it onto a given CRS (hence why we need to specify the CRS in the line below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0b6b290-e73e-4326-8a2d-cb39380695fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        geometry\n",
      "0      POINT (0.00000 -90.00000)\n",
      "1      POINT (1.25000 -90.00000)\n",
      "2      POINT (2.50000 -90.00000)\n",
      "3      POINT (3.75000 -90.00000)\n",
      "4      POINT (5.00000 -90.00000)\n",
      "...                          ...\n",
      "55291  POINT (-6.25000 90.00000)\n",
      "55292  POINT (-5.00000 90.00000)\n",
      "55293  POINT (-3.75000 90.00000)\n",
      "55294  POINT (-2.50000 90.00000)\n",
      "55295  POINT (-1.25000 90.00000)\n",
      "\n",
      "[55296 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create a GeoDataFrame from the xarray dataset's coordinates\n",
    "points_gdf = gpd.GeoDataFrame(geometry = points, crs = 'EPSG:4326')\n",
    "\n",
    "# Print the points to see what they look like\n",
    "print(points_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aac859-603d-414a-88c5-c5a9ba4fa6e1",
   "metadata": {},
   "source": [
    "Here's where the magic happens now! We already have the California state boundaries read into Geopandas - so we can do a [_spatial join_](https://geopandas.org/en/stable/docs/reference/api/geopandas.sjoin.html) on the shapefile and the set of points, which compares each point in the list to the shape to see if it's inside or outside. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03ff1aa4-fc8d-4d15-ae3d-fbf23a65bdeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Spatial join to find points within the shapefile\n",
    "joined = gpd.sjoin(points_gdf, gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03018dc8-4139-4949-9e38-b1e0334679c0",
   "metadata": {},
   "source": [
    "The output of this spatial join (the `joined` variable above) is the set of indices in the `points_gdf` list that are inside the shapefile!\n",
    "\n",
    "Now we know which points we want to keep - but we still need to apply that knowledge to the xarray dataset. We can do this using the code below! This creates a _logical mask_ (True/False) having the same size as the original lat/lon grid, but where every value outside the shape file is False and every value inside is True.\n",
    "\n",
    "After the mask is created, we can apply it to our original xarray dataset using `where`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebd679bb-f6fc-40f3-a17b-6d4472403da9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identify indices of \"good\" data\n",
    "# total number of points\n",
    "num_points = points_gdf.shape[0]\n",
    "\n",
    "# make an array of indices of length num_points\n",
    "inds_array = np.arange(num_points)\n",
    "\n",
    "# Make a logical mask that tells you whether or not \n",
    "# that index is in the set of joined points\n",
    "mask = np.isin(inds_array, joined.index)\n",
    "\n",
    "# Reshape to the original shape of the lat/lon grid\n",
    "mask_2d = mask.reshape(lat2d.shape)\n",
    "\n",
    "# Apply mask to the data\n",
    "masked_data = ens_data.where(mask_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dbb6db-9545-4b5c-840d-28ed63f3e6a7",
   "metadata": {},
   "source": [
    "This all seems fine in theory... but did it actually work?? \n",
    "\n",
    "Let's find out by doing a sanity check: we'll just plot the data and make sure that only the masked values show up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78910081-4050-4a80-a5ab-206207b39a22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define object containing PlateCarree projection\n",
    "map = ccrs.PlateCarree()\n",
    "\n",
    "# Create figure/axis objects, use the map object to specify associated projection\n",
    "fig, ax = plt.subplots(figsize=(20, 10), subplot_kw={\"projection\": map})\n",
    "\n",
    "# Plot temperature data on the axes using the coolwarm colormap\n",
    "plot = ax.pcolormesh(masked_data.lon, masked_data.lat, masked_data.isel(member = 0).mean(dim = 'time').tas, transform = map)\n",
    "\n",
    "# Set lat/lon extent\n",
    "ax.set_extent([-130, -100, 25, 50])\n",
    "# Add colorbar and label it\n",
    "cbar = plt.colorbar(plot, ax=ax)\n",
    "cbar.set_label(\"Temperature (C)\")\n",
    "# Add coastline/border lines\n",
    "ax.add_feature(cfeature.COASTLINE)\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=\":\")\n",
    "ax.add_feature(cfeature.STATES)\n",
    "# Add grid lines\n",
    "gl = ax.gridlines(draw_labels=True, linestyle=\"--\") \n",
    "gl.top_labels = False\n",
    "# Add title, show plot\n",
    "ax.set_title(\"Masked temperature data\", fontsize= 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13322d74-3aed-4e6b-a4a2-b26da6d5d7fd",
   "metadata": {},
   "source": [
    "<a id='time_series'></a> \n",
    "## **Plot a Time Series**\n",
    "\n",
    "Great! Now that we've gotten the shape file masking out of the way, we can go through the steps outlined in tutorial 1 to compute a weighted average and plot the ensemble mean and ensemble spread.\n",
    "\n",
    "First we define a function to generate weights for each data point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f485db8-6c04-409b-b6f3-b70653f0fc18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define function to generate area weights\n",
    "def weights(dat):\n",
    "    # Calculate weighting factor = cosine of latitude\n",
    "    coslat = np.cos(np.deg2rad(dat.lat))\n",
    "    weight_factor = coslat / coslat.mean(dim='lat')\n",
    "    \n",
    "    # Weight all points by the weighting factor\n",
    "    computed_weight = dat * weight_factor\n",
    "    \n",
    "    # Return the set of weights: this has dimension equal to that of the input data\n",
    "    return computed_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9463b872-42b3-4e2f-8a6a-53cf5eb1cdfd",
   "metadata": {},
   "source": [
    "Then we apply the weights, average over lat and lon, and compute annual means to make the plot look prettier later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c02a968-0226-4c7c-b4d5-c0489bbe3a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight the data by grid box area\n",
    "dat_weighted = weights(masked_data)\n",
    "\n",
    "# Average over lat, lon\n",
    "dat_wgtmn = dat_weighted.mean(dim=['lat', 'lon'])\n",
    "\n",
    "# Calculate annual mean\n",
    "dat_wgtmn = dat_wgtmn.groupby('time.year').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919dd2ce-bf7e-4059-8f2e-5129b09bca54",
   "metadata": {},
   "source": [
    "Extracting the values from the xarray dataset makes things go a lot faster later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c979d359-a6e6-4e05-ab98-c27fdaa3afb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the values of the weighted temperature array to speed things up\n",
    "tas = dat_wgtmn.tas.values\n",
    "\n",
    "# Store the number of ensemble members\n",
    "nmems = tas.shape[1]\n",
    "\n",
    "# Also extract the ensemble member names\n",
    "memnames = dat_wgtmn.member.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4bc267-b820-4527-9f66-ba0f470f154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ensemble mean\n",
    "\n",
    "# Ensemble standard deviation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f8eebf-2db9-440c-bfaa-3c3e97ef542b",
   "metadata": {},
   "source": [
    "Then we can generate our plot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490ec012-ff71-452a-be21-089705a03116",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1) Ensemble standard deviation\n",
    "\n",
    "# Set up a blank figure for plotting\n",
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "\n",
    "# Loop over all ensemble members, plot the time series for each one\n",
    "\n",
    "# Set plotting parameters\n",
    "ax.set_title(\"Time Series of CA Near-Surface Air Temperature \", fontsize=20)\n",
    "ax.set_xlabel(\"Year\", fontsize=20)\n",
    "ax.set_ylabel(\"Temperature (C)\", fontsize=20)\n",
    "ax.set_xlim([1850, 2030])\n",
    "ax.legend(fontsize=20)\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d831402-a9bc-4119-a820-a31c7a0af8d4",
   "metadata": {},
   "source": [
    "You're all done! Having the ability to filter geospatial data using shape files is handy, because a lot of the regions you'll encounter aren't precise rectangles that can be spelled out by a box in lat and lon. Hopefully you find this helpful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299e0421-15e2-4c97-976a-6fd7aa359204",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda 3 (EDS296)",
   "language": "python",
   "name": "eds196-stevenson"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
